{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12234a47",
   "metadata": {},
   "source": [
    "# Prompt Engineering Mastery Demo\n",
    "\n",
    "Welcome to this comprehensive demonstration of prompt engineering techniques! This notebook showcases how careful prompt design can dramatically improve AI model outputs.\n",
    "\n",
    "## Table of Contents\n",
    "1. **Before & After Comparison Demo**\n",
    "2. **Interactive Prompt Engineering Workshop** \n",
    "3. **Role-Playing Prompt Demo**\n",
    "4. **Chain-of-Thought vs Direct Prompting**\n",
    "5. **Few-Shot Learning Examples**\n",
    "6. **Common Prompt Engineering Pitfalls**\n",
    "\n",
    "Each section includes practical examples you can run and modify to see the impact of different prompting strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cac2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import openai\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Azure OpenAI Configuration\n",
    "AZURE_OPENAI_API_KEY = \"AZURE_OPENAI_API_KEY_placeholder\" \n",
    "AZURE_OPENAI_ENDPOINT = \"endpoint_placeholder\"  \n",
    "\n",
    "# Helper function to call Azure OpenAI GPT model\n",
    "def call_gpt(prompt: str, max_tokens: int = 500, temperature: float = 0.7) -> str:\n",
    "    \"\"\"\n",
    "    Helper function to call Azure OpenAI GPT model with a prompt\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"api-key\": AZURE_OPENAI_API_KEY\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "        \n",
    "        response = requests.post(AZURE_OPENAI_ENDPOINT, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        result = response.json()\n",
    "        return result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Request Error: {str(e)}\"\n",
    "    except KeyError as e:\n",
    "        return f\"Response parsing error: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        return f\"Unexpected error: {str(e)}\"\n",
    "\n",
    "print(\"Setup complete! Azure OpenAI configured and ready to demonstrate prompt engineering techniques.\")\n",
    "print(\"Using endpoint: https://AIAgent-openai02.openai.azure.com\")\n",
    "print(\"Model: GPT-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc57ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick API Connection Test\n",
    "print(\"Testing Azure OpenAI connection...\")\n",
    "test_response = call_gpt(\"Say 'Hello! The API is working correctly.' in a friendly way.\")\n",
    "print(\"API Response:\", test_response)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"If you see a response above, your Azure OpenAI is configured correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc42d3e",
   "metadata": {},
   "source": [
    "## 1. Before & After Comparison Demo\n",
    "\n",
    "This section demonstrates the dramatic difference between poorly constructed and well-engineered prompts. We'll use the same task but with vastly different prompt quality.\n",
    "\n",
    "### Task: Generate a product description for a smartphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aa8c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POOR PROMPT - Vague and unclear\n",
    "poor_prompt = \"Write about a phone\"\n",
    "\n",
    "print(\"POOR PROMPT:\")\n",
    "print(f\"'{poor_prompt}'\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "poor_response = call_gpt(poor_prompt)\n",
    "print(\"Response:\", poor_response)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# OPTIMIZED PROMPT - Clear, specific, structured\n",
    "optimized_prompt = \"\"\"\n",
    "Write a compelling product description for a premium smartphone targeting tech enthusiasts. \n",
    "\n",
    "Requirements:\n",
    "- Target audience: Tech-savvy professionals aged 25-40\n",
    "- Tone: Professional yet exciting\n",
    "- Length: 100-150 words\n",
    "- Include: Key features, benefits, and a call-to-action\n",
    "- Format: Use bullet points for features\n",
    "\n",
    "Key specifications to highlight:\n",
    "- 6.7\" OLED display with 120Hz refresh rate\n",
    "- Triple camera system with 108MP main sensor\n",
    "- 5000mAh battery with wireless charging\n",
    "- 12GB RAM, 256GB storage\n",
    "- 5G connectivity\n",
    "\"\"\"\n",
    "\n",
    "print(\"OPTIMIZED PROMPT:\")\n",
    "print(f\"'{optimized_prompt}'\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "optimized_response = call_gpt(optimized_prompt)\n",
    "print(\"Response:\", optimized_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdbd50c",
   "metadata": {},
   "source": [
    "## 2. Interactive Prompt Engineering Workshop\n",
    "\n",
    "This section shows progressive prompt refinement - starting with a basic prompt and gradually improving it. We'll tackle a customer service scenario.\n",
    "\n",
    "### Scenario: Handling a Customer Complaint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer complaint scenario\n",
    "customer_complaint = \"\"\"\n",
    "\"I ordered a laptop 2 weeks ago and it still hasn't arrived! \n",
    "This is unacceptable. I want a full refund and compensation for the delay!\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"Customer Complaint:\", customer_complaint)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Version 1: Basic prompt\n",
    "prompt_v1 = f\"Respond to this customer complaint: {customer_complaint}\"\n",
    "\n",
    "print(\"PROMPT VERSION 1 (Basic):\")\n",
    "print(prompt_v1)\n",
    "print(\"\\nResponse:\")\n",
    "print(call_gpt(prompt_v1))\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Version 2: Adding context and tone\n",
    "prompt_v2 = f\"\"\"\n",
    "You are a professional customer service representative. Respond to this customer complaint \n",
    "with empathy and provide a solution: {customer_complaint}\n",
    "\n",
    "Use a helpful, understanding tone.\n",
    "\"\"\"\n",
    "\n",
    "print(\"PROMPT VERSION 2 (Added Context & Tone):\")\n",
    "print(prompt_v2)\n",
    "print(\"\\nResponse:\")\n",
    "print(call_gpt(prompt_v2))\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Version 3: Structured response with specific instructions\n",
    "prompt_v3 = f\"\"\"\n",
    "You are a professional customer service representative for TechCorp. \n",
    "Respond to this customer complaint following this structure:\n",
    "\n",
    "1. Acknowledge the customer's frustration with empathy\n",
    "2. Apologize for the delay without making excuses\n",
    "3. Provide a specific solution (expedited shipping + 20% discount)\n",
    "4. Give a timeline for resolution\n",
    "5. Offer additional support contact information\n",
    "\n",
    "Customer complaint: {customer_complaint}\n",
    "\n",
    "Tone: Professional, empathetic, solution-focused\n",
    "Length: 3-4 sentences per point\n",
    "\"\"\"\n",
    "\n",
    "print(\"PROMPT VERSION 3 (Fully Structured):\")\n",
    "print(prompt_v3)\n",
    "print(\"\\nResponse:\")\n",
    "print(call_gpt(prompt_v3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1e3cd5",
   "metadata": {},
   "source": [
    "## 3. Role-Playing Prompt Demo\n",
    "\n",
    "This section demonstrates how assigning specific roles to AI can dramatically change the perspective and quality of responses. We'll ask the same question from different professional viewpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d12e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same question, different roles\n",
    "question = \"Should our company invest in artificial intelligence?\"\n",
    "\n",
    "roles = {\n",
    "    \"CEO\": \"You are a CEO focused on business growth, profitability, and competitive advantage.\",\n",
    "    \"CTO\": \"You are a Chief Technology Officer with deep technical expertise and understanding of implementation challenges.\",\n",
    "    \"CFO\": \"You are a Chief Financial Officer focused on costs, ROI, and financial risks.\",\n",
    "    \"HR Director\": \"You are an HR Director concerned about employee impact, training needs, and workforce changes.\",\n",
    "    \"Ethics Consultant\": \"You are an AI ethics consultant focused on responsible AI implementation and societal impact.\"\n",
    "}\n",
    "\n",
    "for role, description in roles.items():\n",
    "    prompt = f\"\"\"\n",
    "    {description}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Provide your perspective in 2-3 sentences, focusing on your area of expertise.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"{role.upper()} PERSPECTIVE:\")\n",
    "    print(\"Prompt:\", prompt.strip())\n",
    "    print(\"Response:\", call_gpt(prompt))\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662ed47e",
   "metadata": {},
   "source": [
    "## 4. Chain-of-Thought vs Direct Prompting\n",
    "\n",
    "Chain-of-Thought prompting encourages the AI to break down complex problems into steps, leading to more accurate and reasoned responses. Let's compare direct answers with step-by-step reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a94c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: Planning a marketing campaign budget\n",
    "problem = \"\"\"\n",
    "A startup has $50,000 for marketing. They want to allocate it across:\n",
    "- Social media ads (should be 40% of budget)\n",
    "- Content creation (should be 30% of budget) \n",
    "- Influencer partnerships (should be 20% of budget)\n",
    "- Email marketing tools (remaining budget)\n",
    "\n",
    "They also want to save 15% of the total budget as contingency.\n",
    "How should they allocate their budget?\n",
    "\"\"\"\n",
    "\n",
    "print(\"PROBLEM:\", problem)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Direct prompting\n",
    "direct_prompt = f\"\"\"\n",
    "{problem}\n",
    "\n",
    "Provide the budget allocation.\n",
    "\"\"\"\n",
    "\n",
    "print(\"DIRECT PROMPTING:\")\n",
    "print(\"Prompt:\", direct_prompt.strip())\n",
    "print(\"Response:\", call_gpt(direct_prompt))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Chain-of-Thought prompting\n",
    "cot_prompt = f\"\"\"\n",
    "{problem}\n",
    "\n",
    "Let's work through this step by step:\n",
    "\n",
    "Step 1: Calculate the contingency amount\n",
    "Step 2: Determine the available budget after contingency\n",
    "Step 3: Calculate each category allocation\n",
    "Step 4: Verify the calculations\n",
    "Step 5: Present the final budget breakdown\n",
    "\n",
    "Please show your work for each step.\n",
    "\"\"\"\n",
    "\n",
    "print(\"CHAIN-OF-THOUGHT PROMPTING:\")\n",
    "print(\"Prompt:\", cot_prompt.strip())\n",
    "print(\"Response:\", call_gpt(cot_prompt, max_tokens=800))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ca324",
   "metadata": {},
   "source": [
    "## 5. Few-Shot Learning Examples\n",
    "\n",
    "Few-shot learning provides the AI with examples of the desired input-output format, dramatically improving consistency and quality. We'll demonstrate this with a text classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c55420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Classify customer feedback sentiment and extract key themes\n",
    "new_feedback = \"The delivery was super fast and the packaging was great, but the product quality was disappointing. The customer service team was helpful though.\"\n",
    "\n",
    "# Zero-shot approach\n",
    "zero_shot_prompt = f\"\"\"\n",
    "Classify the sentiment and extract key themes from this customer feedback:\n",
    "\"{new_feedback}\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"ZERO-SHOT APPROACH:\")\n",
    "print(\"Prompt:\", zero_shot_prompt)\n",
    "print(\"Response:\", call_gpt(zero_shot_prompt))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Few-shot approach with examples\n",
    "few_shot_prompt = f\"\"\"\n",
    "Classify customer feedback sentiment and extract key themes. Use this format:\n",
    "\n",
    "Example 1:\n",
    "Feedback: \"Love the product but shipping took forever!\"\n",
    "Sentiment: Mixed (Positive: product quality, Negative: shipping speed)\n",
    "Themes: Product satisfaction, Delivery issues\n",
    "\n",
    "Example 2:\n",
    "Feedback: \"Excellent customer service, quick response to my question.\"\n",
    "Sentiment: Positive\n",
    "Themes: Customer service excellence, Response time\n",
    "\n",
    "Example 3:\n",
    "Feedback: \"Product broke after one week, very poor quality.\"\n",
    "Sentiment: Negative\n",
    "Themes: Product durability, Quality issues\n",
    "\n",
    "Now classify this feedback:\n",
    "Feedback: \"{new_feedback}\"\n",
    "Sentiment: \n",
    "Themes:\n",
    "\"\"\"\n",
    "\n",
    "print(\"FEW-SHOT APPROACH:\")\n",
    "print(\"Prompt:\", few_shot_prompt)\n",
    "print(\"Response:\", call_gpt(few_shot_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d681d81c",
   "metadata": {},
   "source": [
    "## 6. Common Prompt Engineering Pitfalls\n",
    "\n",
    "This section demonstrates common mistakes in prompt engineering and how to fix them. Learning what NOT to do is just as important as learning best practices!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b539ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common pitfalls and their fixes\n",
    "pitfalls = {\n",
    "    \"Ambiguous Instructions\": {\n",
    "        \"bad\": \"Make it better\",\n",
    "        \"good\": \"Improve the readability of this code by adding comments, using descriptive variable names, and breaking long functions into smaller ones.\",\n",
    "        \"issue\": \"Too vague - 'better' could mean anything\"\n",
    "    },\n",
    "    \n",
    "    \"Leading Questions\": {\n",
    "        \"bad\": \"Don't you think Python is the best programming language for data science?\",\n",
    "        \"good\": \"Compare Python, R, and Julia for data science projects. Consider factors like ecosystem, performance, and learning curve.\",\n",
    "        \"issue\": \"Biases the response toward Python\"\n",
    "    },\n",
    "    \n",
    "    \"Missing Context\": {\n",
    "        \"bad\": \"Fix this bug\",\n",
    "        \"good\": \"This Python function should calculate compound interest but returns incorrect values. The issue seems to be in the formula. Here's the code: [code]. Please identify and fix the bug.\",\n",
    "        \"issue\": \"No context about what needs fixing or what the expected behavior is\"\n",
    "    },\n",
    "    \n",
    "    \"Too Many Tasks\": {\n",
    "        \"bad\": \"Analyze this data, create visualizations, write a report, suggest improvements, and predict future trends\",\n",
    "        \"good\": \"Analyze this sales data and identify the top 3 trends. Focus on seasonal patterns and product performance.\",\n",
    "        \"issue\": \"Trying to do too much in one prompt leads to superficial results\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for pitfall_name, details in pitfalls.items():\n",
    "    print(f\"PITFALL: {pitfall_name}\")\n",
    "    print(f\"Issue: {details['issue']}\")\n",
    "    print(f\"Bad prompt: \\\"{details['bad']}\\\"\")\n",
    "    print(f\"Good prompt: \\\"{details['good']}\\\"\")\n",
    "    print(f\"Bad response: {call_gpt(details['bad'])}\")\n",
    "    print(f\"Good response: {call_gpt(details['good'])}\")\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfd0aea",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What Makes a Great Prompt?\n",
    "\n",
    "1. **Clear and Specific**: Define exactly what you want\n",
    "2. **Provide Context**: Give background information and constraints\n",
    "3. **Use Examples**: Show the AI what good output looks like\n",
    "4. **Structure Your Request**: Break complex tasks into steps\n",
    "5. **Define the Role**: Tell the AI what perspective to take\n",
    "6. **Specify Format**: Describe how you want the output structured\n",
    "\n",
    "### Interactive Exercise\n",
    "Try modifying the prompts above and see how the responses change! Use the testing section below to experiment with your own prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc66f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Prompt Testing\n",
    "# Use this cell to test your own prompts!\n",
    "\n",
    "def test_prompt(prompt: str, description: str = \"\"):\n",
    "    \"\"\"Test a custom prompt and display the results\"\"\"\n",
    "    print(f\"TESTING: {description}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {call_gpt(prompt)}\")\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Example: Test your own prompt\n",
    "your_prompt = \"Explain quantum computing to a 10-year-old using analogies\"\n",
    "test_prompt(your_prompt, \"Custom Prompt Example\")\n",
    "\n",
    "# TODO: Replace with your own prompts to test\n",
    "# test_prompt(\"Your prompt here\", \"Your description here\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
